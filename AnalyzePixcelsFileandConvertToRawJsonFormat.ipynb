{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMqPW/wviZ8WsRFxrOmilyO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sssangeetha/OutamationAI_OCR_RAG_Automation/blob/main/AnalyzePixcelsFileandConvertToRawJsonFormat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1ASvwoo7fhI",
        "outputId": "3f886454-1d3e-49b2-fe6f-48e79244ddfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.12/dist-packages (1.26.5)\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.12/dist-packages (0.3.13)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from pytesseract) (25.0)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from pytesseract) (11.3.0)\n",
            "Requirement already satisfied: dateparser in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.12/dist-packages (from dateparser) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2024.2 in /usr/local/lib/python3.12/dist-packages (from dateparser) (2025.2)\n",
            "Requirement already satisfied: regex>=2024.9.11 in /usr/local/lib/python3.12/dist-packages (from dateparser) (2024.11.6)\n",
            "Requirement already satisfied: tzlocal>=0.2 in /usr/local/lib/python3.12/dist-packages (from dateparser) (5.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7.0->dateparser) (1.17.0)\n",
            "Saved out.json\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "import os, sys, json, math, argparse, re, io\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import List, Tuple, Dict, Any, Optional\n",
        "\n",
        "!pip install pymupdf\n",
        "%pip install pytesseract\n",
        "%pip install dateparser\n",
        "import fitz\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "import regex as rxx\n",
        "import dateparser\n",
        "\n",
        "# -----------------------\n",
        "# Helpers & data classes\n",
        "# -----------------------\n",
        "\n",
        "@dataclass\n",
        "class Token:\n",
        "    text: str\n",
        "    conf: float\n",
        "    bbox: Tuple[int,int,int,int]  # x0,y0,x1,y1\n",
        "    block_num: int\n",
        "    par_num: int\n",
        "    line_num: int\n",
        "    word_num: int\n",
        "\n",
        "@dataclass\n",
        "class Field:\n",
        "    value: str\n",
        "    page: int\n",
        "    bbox: Tuple[int,int,int,int]\n",
        "    confidence: float\n",
        "\n",
        "def clamp(v, lo, hi): return max(lo, min(hi, v))\n",
        "\n",
        "def to_int_bbox(x, y, w, h):\n",
        "    return (int(x), int(y), int(x+w), int(y+h))\n",
        "\n",
        "def union_bbox(b1, b2):\n",
        "    x0 = min(b1[0], b2[0]); y0 = min(b1[1], b2[1])\n",
        "    x1 = max(b1[2], b2[2]); y1 = max(b1[3], b2[3])\n",
        "    return (x0,y0,x1,y1)\n",
        "\n",
        "def bbox_area(b):\n",
        "    return max(0, b[2]-b[0]) * max(0, b[3]-b[1])\n",
        "\n",
        "# common OCR confusions\n",
        "CONFUSION_MAP = {\n",
        "    \"0\":\"O\", \"O\":\"0\",\n",
        "    \"1\":\"I\", \"I\":\"1\", \"l\":\"1\",\n",
        "    \"5\":\"S\", \"S\":\"5\",\n",
        "    \"8\":\"B\", \"B\":\"8\"\n",
        "}\n",
        "\n",
        "def clean_ocr_text(s: str) -> str:\n",
        "    # basic normalization + confusion fixes for isolated tokens\n",
        "    s2 = s.replace('\\u2014','-').replace('\\u2013','-').replace('\\u00A0',' ')\n",
        "    s2 = rxx.sub(r\"[^\\x20-\\x7E]\", \"\", s2)  # strip non-ascii controls (keep it simple)\n",
        "    # Token-level swap when token is short (likely confused char)\n",
        "    toks = s2.split()\n",
        "    out = []\n",
        "    for t in toks:\n",
        "        if len(t) == 1 and t in CONFUSION_MAP:\n",
        "            out.append(CONFUSION_MAP[t])\n",
        "        else:\n",
        "            out.append(t)\n",
        "    return \" \".join(out)\n",
        "\n",
        "# ---------------\n",
        "# Pre-processing\n",
        "# ---------------\n",
        "def preprocess_for_ocr(img_bgr: np.ndarray, max_deskew_angle: float=7.0) -> np.ndarray:\n",
        "    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # denoise\n",
        "    den = cv2.fastNlMeansDenoising(gray, h=15, templateWindowSize=7, searchWindowSize=21)\n",
        "\n",
        "    # adaptive threshold\n",
        "    thr = cv2.adaptiveThreshold(den, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "                                cv2.THRESH_BINARY, 35, 15)\n",
        "\n",
        "    # morphological opening to remove small noise\n",
        "    kernel = np.ones((2,2), np.uint8)\n",
        "    opened = cv2.morphologyEx(thr, cv2.MORPH_OPEN, kernel, iterations=1)\n",
        "\n",
        "    # deskew (estimate skew via minAreaRect of edges)\n",
        "    edges = cv2.Canny(opened, 50, 150)\n",
        "    coords = np.column_stack(np.where(edges > 0))\n",
        "    if coords.size > 0:\n",
        "        rect = cv2.minAreaRect(coords[:, ::-1])\n",
        "        angle = rect[-1]\n",
        "        if angle < -45:\n",
        "            angle = 90 + angle\n",
        "        # limit extreme rotations\n",
        "        angle = float(clamp(angle, -max_deskew_angle, max_deskew_angle))\n",
        "        (h, w) = opened.shape[:2]\n",
        "        M = cv2.getRotationMatrix2D((w//2, h//2), angle, 1.0)\n",
        "        deskewed = cv2.warpAffine(opened, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)\n",
        "    else:\n",
        "        deskewed = opened\n",
        "\n",
        "    return deskewed\n",
        "\n",
        "# ---------------\n",
        "# OCR\n",
        "# ---------------\n",
        "def ocr_page(img_bin: np.ndarray, psm:int=6) -> List[Token]:\n",
        "    # Use image_to_data for tokens + bboxes\n",
        "    config = f\"--oem 3 --psm {psm}\"\n",
        "    data = pytesseract.image_to_data(img_bin, output_type=pytesseract.Output.DICT, config=config)\n",
        "    tokens = []\n",
        "    n = len(data[\"text\"])\n",
        "    for i in range(n):\n",
        "        txt = data[\"text\"][i]\n",
        "        if txt is None or txt.strip() == \"\":\n",
        "            continue\n",
        "        conf = float(data[\"conf\"][i]) if data[\"conf\"][i] != '-1' else 0.0\n",
        "        x, y, w, h = data[\"left\"][i], data[\"top\"][i], data[\"width\"][i], data[\"height\"][i]\n",
        "        tokens.append(Token(\n",
        "            text=clean_ocr_text(txt),\n",
        "            conf=conf/100.0,\n",
        "            bbox=to_int_bbox(x,y,w,h),\n",
        "            block_num=int(data[\"block_num\"][i]),\n",
        "            par_num=int(data[\"par_num\"][i]),\n",
        "            line_num=int(data[\"line_num\"][i]),\n",
        "            word_num=int(data[\"word_num\"][i]),\n",
        "        ))\n",
        "    return tokens\n",
        "\n",
        "# -----------------------------\n",
        "# Extraction Heuristics/Regex\n",
        "# -----------------------------\n",
        "def load_patterns(cfg_path: str) -> Dict[str, Any]:\n",
        "    with open(cfg_path, \"r\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def tokens_in_region(tokens: List[Token], x0,y0,x1,y1) -> List[Token]:\n",
        "    out = []\n",
        "    for t in tokens:\n",
        "        bx0,by0,bx1,by1 = t.bbox\n",
        "        if bx0 >= x0 and by0 >= y0 and bx1 <= x1 and by1 <= y1:\n",
        "            out.append(t)\n",
        "    return out\n",
        "\n",
        "def find_title(tokens: List[Token], page_w: int, page_h: int, hints: List[str]) -> Optional[Field]:\n",
        "    # Look at the top 25% of page for big all-caps words that match hints\n",
        "    top_region = (0, 0, page_w, int(page_h*0.25))\n",
        "    region = tokens_in_region(tokens, *top_region)\n",
        "    text = \" \".join([t.text for t in region])\n",
        "    best = None\n",
        "    for h in hints:\n",
        "        # prioritize full word match\n",
        "        pat = re.compile(rf\"\\b{re.escape(h)}\\b\", re.IGNORECASE)\n",
        "        m = pat.search(text)\n",
        "        if m:\n",
        "            # approximate bbox from all tokens in region that match the hint\n",
        "            hit_tokens = [t for t in region if h.split()[0].lower() in t.text.lower()]\n",
        "            if hit_tokens:\n",
        "                b = hit_tokens[0].bbox\n",
        "                for t in hit_tokens[1:]:\n",
        "                    b = union_bbox(b, t.bbox)\n",
        "                best = Field(value=h.title(), page=1, bbox=b, confidence=0.9)\n",
        "                break\n",
        "    return best\n",
        "\n",
        "def nearest_value_after_keyword(tokens: List[Token], keyword_list: List[str], window_tokens:int=12) -> Optional[Field]:\n",
        "    # Find keyword; then take next N tokens that look like a name (ALL CAPS words) or a phrase ending by comma/newline\n",
        "    for i,t in enumerate(tokens):\n",
        "        for kw in keyword_list:\n",
        "            if kw.lower() in t.text.lower():\n",
        "                # scan ahead\n",
        "                seq = tokens[i+1:i+1+window_tokens]\n",
        "                # collect consecutive title-cased/upper tokens as a name\n",
        "                name_toks = []\n",
        "                for s in seq:\n",
        "                    if re.match(r\"^[A-Z][A-Z\\-\\.',]*$|^[A-Z][a-z\\-\\.',]+$\", s.text) and len(s.text) > 1:\n",
        "                        name_toks.append(s)\n",
        "                    elif name_toks:\n",
        "                        break\n",
        "                if name_toks:\n",
        "                    val = \" \".join([s.text for s in name_toks])\n",
        "                    b = name_toks[0].bbox\n",
        "                    for s in name_toks[1:]:\n",
        "                        b = union_bbox(b, s.bbox)\n",
        "                    conf = np.mean([s.conf for s in name_toks]).item()\n",
        "                    return Field(value=val, page=1, bbox=b, confidence=float(conf))\n",
        "    return None\n",
        "\n",
        "def find_currency(tokens: List[Token], currency_regex: str, hints: List[str]) -> Optional[Field]:\n",
        "    joined = \" \".join([t.text for t in tokens])\n",
        "    m = re.search(currency_regex, joined)\n",
        "    if m:\n",
        "        # approximate bbox from participating tokens\n",
        "        start_idx = None\n",
        "        end_idx = None\n",
        "        span_txt = m.group(0)\n",
        "        # find tokens that cover the span\n",
        "        for i,t in enumerate(tokens):\n",
        "            if start_idx is None and span_txt.startswith(t.text):\n",
        "                start_idx = i\n",
        "            if start_idx is not None and ''.join([x.text for x in tokens[start_idx:i+1]]).startswith(span_txt.replace(\" \", \"\")):\n",
        "                end_idx = i\n",
        "                break\n",
        "        if start_idx is not None and end_idx is not None:\n",
        "            bb = tokens[start_idx].bbox\n",
        "            for s in tokens[start_idx+1:end_idx+1]:\n",
        "                bb = union_bbox(bb, s.bbox)\n",
        "            conf = np.mean([s.conf for s in tokens[start_idx:end_idx+1]]).item()\n",
        "            return Field(value=span_txt, page=1, bbox=bb, confidence=float(conf))\n",
        "    # fallback: look near hint keywords\n",
        "    for i,t in enumerate(tokens):\n",
        "        for kw in hints:\n",
        "            if kw.lower() in t.text.lower():\n",
        "                seq = tokens[i:i+15]\n",
        "                joined2 = \" \".join([x.text for x in seq])\n",
        "                m2 = re.search(currency_regex, joined2)\n",
        "                if m2:\n",
        "                    idxs = list(range(i, min(i+15, len(tokens))))\n",
        "                    bb = tokens[idxs[0]].bbox\n",
        "                    for k in idxs[1:]:\n",
        "                        bb = union_bbox(bb, tokens[k].bbox)\n",
        "                    return Field(value=m2.group(0), page=1, bbox=bb, confidence=0.7)\n",
        "    return None\n",
        "\n",
        "def find_address(tokens: List[Token], addr_regex: str, hints: List[str]) -> Optional[Field]:\n",
        "    joined = \" \".join([t.text for t in tokens])\n",
        "    m = re.search(addr_regex, joined, flags=re.IGNORECASE)\n",
        "    if m:\n",
        "        span = m.group(0)\n",
        "        # crude bbox aggregation over tokens containing parts of span\n",
        "        used = []\n",
        "        for t in tokens:\n",
        "            if any(part in t.text for part in re.split(r\"\\s+\", span) if part):\n",
        "                used.append(t)\n",
        "        if used:\n",
        "            bb = used[0].bbox\n",
        "            for u in used[1:]:\n",
        "                bb = union_bbox(bb, u.bbox)\n",
        "            conf = np.mean([u.conf for u in used]).item()\n",
        "            return Field(value=span, page=1, bbox=bb, confidence=float(conf))\n",
        "    # keyword-based local search\n",
        "    for i,t in enumerate(tokens):\n",
        "        for kw in hints:\n",
        "            if kw.lower() in t.text.lower():\n",
        "                seq = tokens[i:i+25]\n",
        "                txt = \" \".join([x.text for x in seq])\n",
        "                m2 = re.search(addr_regex, txt, flags=re.IGNORECASE)\n",
        "                if m2:\n",
        "                    span = m2.group(0)\n",
        "                    used = seq\n",
        "                    bb = used[0].bbox\n",
        "                    for u in used[1:]:\n",
        "                        bb = union_bbox(bb, u.bbox)\n",
        "                    return Field(value=span, page=1, bbox=bb, confidence=0.65)\n",
        "    return None\n",
        "\n",
        "def find_dates(tokens: List[Token], date_regex: str, hints: List[str]) -> Dict[str, Field]:\n",
        "    out = {}\n",
        "    joined = \" \".join([t.text for t in tokens])\n",
        "    for m in re.finditer(date_regex, joined):\n",
        "        raw = m.group(0)\n",
        "        parsed = dateparser.parse(raw)\n",
        "        if not parsed:\n",
        "            continue\n",
        "        # rough heuristic: first date near \"DATED\" is loan_date; first near \"RECORDED\" is recording_date\n",
        "        idx = joined[:m.start()].count(\" \")\n",
        "        # nearest tokens window for bbox\n",
        "        win = tokens[max(0, idx-5):min(len(tokens), idx+5)]\n",
        "        bb = win[0].bbox\n",
        "        for w in win[1:]:\n",
        "            bb = union_bbox(bb, w.bbox)\n",
        "        field = Field(value=raw, page=1, bbox=bb, confidence=0.7)\n",
        "        # classify using neighborhood hints\n",
        "        neigh = \" \".join([w.text for w in win]).upper()\n",
        "        if any(h in neigh for h in [\"RECORDED\", \"RECORDING\"]):\n",
        "            if \"recording_date\" not in out:\n",
        "                out[\"recording_date\"] = field\n",
        "        elif any(h in neigh for h in [\"DATED\", \"DATE\"]):\n",
        "            if \"loan_date\" not in out:\n",
        "                out[\"loan_date\"] = field\n",
        "        else:\n",
        "            # fill first available\n",
        "            out.setdefault(\"loan_date\", field)\n",
        "    return out\n",
        "\n",
        "# -----------------------------\n",
        "# Main pipeline\n",
        "# -----------------------------\n",
        "def pdf_to_images(pdf_path: str, dpi: int=300) -> List[np.ndarray]:\n",
        "    doc = fitz.open(pdf_path)\n",
        "    pages = []\n",
        "    for i, page in enumerate(doc):\n",
        "        mat = fitz.Matrix(dpi/72, dpi/72)\n",
        "        pix = page.get_pixmap(matrix=mat, annots=False)\n",
        "        img = np.frombuffer(pix.samples, dtype=np.uint8).reshape(pix.h, pix.w, pix.n)\n",
        "        if pix.n == 4:\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGRA2BGR)\n",
        "        pages.append(img)\n",
        "    return pages\n",
        "\n",
        "def aggregate_tokens(tokens: List[Token]) -> Dict[str, Any]:\n",
        "    return {\n",
        "        \"tokens\": [{\n",
        "            \"text\": t.text, \"conf\": round(t.conf*100,2), \"bbox\": list(t.bbox),\n",
        "            \"block_id\": t.block_num, \"line_id\": t.line_num, \"word_id\": t.word_num\n",
        "        } for t in tokens]\n",
        "    }\n",
        "\n",
        "def main(pdf_path: str, out_path: str = \"out.json\", patterns_path: str = \"regex_patterns.json\", save_artifacts: bool = False, psm: int = 6, max_deskew_angle: float = 7.0):\n",
        "    cfg = load_patterns(patterns_path)\n",
        "    pages = pdf_to_images(pdf_path)\n",
        "\n",
        "    results = {\n",
        "        \"document_title\": None,\n",
        "        \"borrower_name\": [],\n",
        "        \"lender_name\": [],\n",
        "        \"loan_amount\": None,\n",
        "        \"property_address\": None,\n",
        "        \"dates\": {},\n",
        "        \"pages\": []\n",
        "    }\n",
        "\n",
        "    os.makedirs(\"artifacts\", exist_ok=True) if save_artifacts else None\n",
        "\n",
        "    for pageno, bgr in enumerate(pages, start=1):\n",
        "        bin_img = preprocess_for_ocr(bgr, max_deskew_angle=max_deskew_angle)\n",
        "        tokens = ocr_page(bin_img, psm=psm)\n",
        "\n",
        "        h, w = bin_img.shape[:2]\n",
        "\n",
        "        if save_artifacts:\n",
        "            cv2.imwrite(os.path.join(\"artifacts\", f\"page_{pageno:02d}_preprocessed.png\"), bin_img)\n",
        "            # save token table as CSV\n",
        "            import csv\n",
        "            with open(os.path.join(\"artifacts\", f\"ocr_page_{pageno:02d}.csv\"), \"w\", newline=\"\") as f:\n",
        "                cw = csv.writer(f)\n",
        "                cw.writerow([\"text\",\"conf\",\"x0\",\"y0\",\"x1\",\"y1\",\"block\",\"par\",\"line\",\"word\"])\n",
        "                for t in tokens:\n",
        "                    cw.writerow([t.text, round(t.conf,3), *t.bbox, t.block_num, t.par_num, t.line_num, t.word_num])\n",
        "\n",
        "        # title from top region of page 1 only\n",
        "        if pageno == 1 and results[\"document_title\"] is None:\n",
        "            title = find_title(tokens, w, h, cfg[\"TITLE_HINTS\"])\n",
        "            if title:\n",
        "                results[\"document_title\"] = asdict(title)\n",
        "\n",
        "        # borrower/lender (try each page until found at least once)\n",
        "        if not results[\"borrower_name\"]:\n",
        "            b = nearest_value_after_keyword(tokens, cfg[\"BORROWER_HINTS\"])\n",
        "            if b:\n",
        "                results[\"borrower_name\"].append(asdict(b))\n",
        "        if not results[\"lender_name\"]:\n",
        "            l = nearest_value_after_keyword(tokens, cfg[\"LENDER_HINTS\"])\n",
        "            if l:\n",
        "                results[\"lender_name\"].append(asdict(l))\n",
        "\n",
        "        # loan amount\n",
        "        if results[\"loan_amount\"] is None:\n",
        "            la = find_currency(tokens, cfg[\"CURRENCY_REGEX\"], cfg[\"LOAN_AMOUNT_HINTS\"])\n",
        "            if la:\n",
        "                results[\"loan_amount\"] = asdict(la)\n",
        "\n",
        "        # address\n",
        "        if results[\"property_address\"] is None:\n",
        "            addr = find_address(tokens, cfg[\"ADDRESS_REGEX\"], cfg[\"ADDRESS_HINTS\"])\n",
        "            if addr:\n",
        "                results[\"property_address\"] = asdict(addr)\n",
        "\n",
        "        # dates\n",
        "        if not results[\"dates\"]:\n",
        "            d = find_dates(tokens, cfg[\"DATE_REGEX\"], cfg[\"DATE_HINTS\"] + cfg[\"RECORDING_HINTS\"])\n",
        "            for k,v in d.items():\n",
        "                results[\"dates\"][k] = asdict(v)\n",
        "\n",
        "        # store page tokens\n",
        "        page_entry = {\"page_number\": pageno}\n",
        "        page_entry.update(aggregate_tokens(tokens))\n",
        "        results[\"pages\"].append(page_entry)\n",
        "\n",
        "    # Final sanity tweaks: ensure empties are explicit None / []\n",
        "    results[\"document_title\"] = results[\"document_title\"]\n",
        "    results[\"borrower_name\"] = results[\"borrower_name\"]\n",
        "    results[\"lender_name\"] = results[\"lender_name\"]\n",
        "    results[\"loan_amount\"] = results[\"loan_amount\"]\n",
        "    results[\"property_address\"] = results[\"property_address\"]\n",
        "    results[\"dates\"] = results[\"dates\"]\n",
        "\n",
        "    with open(out_path, \"w\") as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "\n",
        "    print(f\"Saved {out_path}\")\n",
        "    if save_artifacts:\n",
        "        print(\"Artifacts saved in ./artifacts\")\n",
        "\n",
        "# Call main with the path to your PDF file\n",
        "main(pdf_path=\"/content/MTG_10009588.pdf\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7844bad",
        "outputId": "160dc668-8942-4432-db48-a9076b046d10"
      },
      "source": [
        "import json\n",
        "\n",
        "patterns_path = \"/content/regex_patterns.json\"\n",
        "with open(patterns_path, \"r\") as f:\n",
        "    cfg = json.load(f)\n",
        "\n",
        "\n",
        "\n",
        "original_address_regex = cfg.get(\"ADDRESS_REGEX\", \"\")\n",
        "fixed_address_regex = original_address_regex.replace(r\"\\-\", r\"\\\\-\") # Replace `\\-` with `\\\\-`\n",
        "\n",
        "# If the issue is within a character class like `[\\- ]`, try converting to `[- ]`\n",
        "# fixed_address_regex = original_address_regex.replace(\"[\\\\- ]\", \"[- ]\")\n",
        "\n",
        "\n",
        "cfg[\"ADDRESS_REGEX\"] = fixed_address_regex\n",
        "\n",
        "with open(patterns_path, \"w\") as f:\n",
        "    json.dump(cfg, f, indent=2)\n",
        "\n",
        "print(f\"Updated {patterns_path} with modified ADDRESS_REGEX.\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated /content/regex_patterns.json with modified ADDRESS_REGEX.\n"
          ]
        }
      ]
    }
  ]
}